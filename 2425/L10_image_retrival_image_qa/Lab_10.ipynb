{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Setup](#first-bullet)\n",
    "* [Image Retrieval using Multimodal Models](#second-bullet)\n",
    "* [Multimodal Visual QA](#third-bullet)\n",
    "* [Datasets and Metrics](#eigth-bullet)\n",
    "* [TLDR](#last-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ProgramingTools\\anaconda_gus\\envs\\conda_fii_cv_3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Retrieval using Multimodal Models <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimodal models leverage multiple data modalities (e.g., images and text) to achieve advanced tasks like image retrieval and question answering.\n",
    "\n",
    "### Why Multimodal\n",
    "- Combines strengths of different data types.\n",
    "- Enables applications like:\n",
    "  - Image search using text.\n",
    "  - Answering questions about visual content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Image retrieval involves finding the most relevant image(s) from a dataset based on a query. Multimodal retrieval uses text or other images as queries.\n",
    "\n",
    "___\n",
    "\n",
    "### Key Idea\n",
    "Learn a shared embedding space for text and images where semantically related inputs are close.\n",
    "\n",
    "___\n",
    "\n",
    "#### Key Concepts\n",
    "- **Feature Extraction:** Represent images and text in a numerical form.\n",
    "- **Similarity Matching:** Use a distance metric (e.g., cosine similarity) to measure relevance.\n",
    "\n",
    "___\n",
    "\n",
    "### **Key Steps in Image Retrieval**\n",
    "\n",
    "#### **1. Feature Extraction**\n",
    "- Extract numerical representations (embeddings) for both the query and the database images.\n",
    "- Common techniques:\n",
    "  - **For images:** Use Convolutional Neural Networks (CNNs) or Vision Transformers (ViT).\n",
    "  - **For text queries:** Use transformers such as BERT or GPT.\n",
    "\n",
    "#### **2. Embedding Space**\n",
    "- Represent both images and text queries in a **shared embedding space**.\n",
    "- Similar images or matching image-text pairs should be close together in this space.\n",
    "- Distance metrics such as **cosine similarity** or **Euclidean distance** are used to compute similarity.\n",
    "\n",
    "#### **3. Similarity Computation**\n",
    "- Compare the query embedding with embeddings from the database using a similarity metric.\n",
    "- Rank the database images based on their similarity to the query.\n",
    "\n",
    "___\n",
    "\n",
    "### Mathematical Formulation\n",
    "Given:\n",
    "- $ I $: Image embeddings.\n",
    "- $ T $: Text embeddings.\n",
    "\n",
    "Objective: Minimize the loss function $ \\mathcal{L} $:\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(I_i, T_i))}{\\sum_{j=1}^{N} \\exp(\\text{sim}(I_i, T_j))}\n",
    "$$\n",
    "where $ \\text{sim}(I, T) $ is a similarity measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History of Image Retrieval\n",
    "- 2001: Early models used SIFT and traditional machine learning.\n",
    "- 2014: CNNs revolutionized image feature extraction.\n",
    "- 2021: OpenAI's **CLIP** introduced a robust multimodal framework using contrastive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Architecture\n",
    "- **Image Encoder:** ResNet or Vision Transformer (ViT).\n",
    "- **Text Encoder:** Transformer-based architecture (e.g., GPT-2).\n",
    "- **Shared Space:** Project image and text embeddings into a common latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Learning: Concepts and Mathematical Foundation\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Contrastive Learning** is a representation learning technique that aims to map data points into a feature space where similar samples are close to each other and dissimilar samples are far apart. It is widely used in self-supervised learning and other tasks where labeled data is limited.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Representation Learning**:\n",
    "   - Learn a function $f_\\theta$ (parameterized by $\\theta$) that maps input data $x$ into a feature embedding $z = f_\\theta(x)$.\n",
    "   - The goal is to ensure embeddings preserve meaningful semantic relationships.\n",
    "\n",
    "2. **Positive and Negative Pairs**:\n",
    "   - **Positive pairs**: Represent semantically similar examples (e.g., two augmented views of the same image).\n",
    "   - **Negative pairs**: Represent semantically dissimilar examples (e.g., different images or text).\n",
    "\n",
    "3. **Contrastive Loss**:\n",
    "   - A loss function designed to minimize the distance between positive pairs while maximizing the distance between negative pairs.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Feature Space\n",
    "\n",
    "Let:\n",
    "- $x_i$ and $x_j$: Two data points (e.g., augmented versions of an image).\n",
    "- $z_i = f_\\theta(x_i)$ and $z_j = f_\\theta(x_j)$: Their embeddings in the feature space.\n",
    "\n",
    "The embeddings are normalized to lie on a unit hypersphere, i.e., $\\|z_i\\| = 1$.\n",
    "\n",
    "### Similarity Metric\n",
    "\n",
    "The similarity between two embeddings is measured using the **cosine similarity**:\n",
    "$$\n",
    "\\text{sim}(z_i, z_j) = \\frac{z_i^\\top z_j}{\\|z_i\\| \\|z_j\\|}.\n",
    "$$\n",
    "Since embeddings are normalized, this simplifies to:\n",
    "$$\n",
    "\\text{sim}(z_i, z_j) = z_i^\\top z_j.\n",
    "$$\n",
    "\n",
    "### Contrastive Loss\n",
    "\n",
    "#### 1. **Triplet Loss**:\n",
    "Given an anchor $z_a$, a positive sample $z_p$, and a negative sample $z_n$, the triplet loss is:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{triplet}} = \\max(0, \\|z_a - z_p\\|^2 - \\|z_a - z_n\\|^2 + m),\n",
    "$$\n",
    "where $m > 0$ is a margin that separates positive and negative pairs.\n",
    "\n",
    "#### 2. **NT-Xent Loss** (Normalized Temperature-Scaled Cross-Entropy Loss):\n",
    "The NT-Xent loss, used in SimCLR, is defined as:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NT-Xent}} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j) / \\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k) / \\tau)},\n",
    "$$\n",
    "where:\n",
    "- $\\tau > 0$ is a temperature scaling parameter.\n",
    "- $N$ is the batch size.\n",
    "- The numerator represents the similarity between positive pairs $(z_i, z_j)$.\n",
    "- The denominator sums over all pairs, including both positive and negative pairs.\n",
    "\n",
    "#### 3. **Contrastive Loss** (not to be confused with the triplet, although there are similarities)\n",
    "$$\n",
    "\\mathcal{L}_{\\text{contrastive}} = \\frac{1}{N} \\sum_{i=1}^N \\bigg( y_{i} \\cdot \\|z_i - z_j\\|^2 + (1 - y_{i}) \\cdot \\max(0, m - \\|z_i - z_j\\|)^2 \\bigg),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$: Number of pairs.\n",
    "- $y_i \\in \\{0, 1\\}$: Binary label indicating whether the pair is positive ($y_i = 1$) or negative ($y_i = 0$).\n",
    "- $\\|z_i - z_j\\|$: The distance (e.g., Euclidean) between the embeddings of the two samples.\n",
    "- $m$: Margin that defines the minimum allowable distance between negative pairs.\n",
    "- The first term ($y_i \\cdot \\|z_i - z_j\\|^2$) minimizes the distance for positive pairs.\n",
    "- The second term ($(1 - y_i) \\cdot \\max(0, m - \\|z_i - z_j\\|)^2$) ensures negative pairs are separated by at least the margin $m$.\n",
    "\n",
    "##### Intuition\n",
    "- For **positive pairs** ($y_i = 1$): Minimize the squared distance $\\|z_i - z_j\\|^2$ so that similar samples are closer in the embedding space.\n",
    "- For **negative pairs** ($y_i = 0$): Ensure that their distance is greater than the margin $m$. If the distance is already greater than $m$, the loss contributes $0$ for that pair.\n",
    "\n",
    "### Optimization Goal\n",
    "\n",
    "The goal of contrastive learning is to minimize the loss function so that positive pairs are closer together and negative pairs are farther apart.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "\n",
    "1. **Self-Supervised Learning**:\n",
    "   - SimCLR and MoCo use contrastive loss to pretrain models without labeled data.\n",
    "2. **Image Retrieval**:\n",
    "   - Representations learned can be used to retrieve semantically similar images.\n",
    "3. **Multimodal Learning**:\n",
    "   - CLIP aligns text and image embeddings using contrastive learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Reduces reliance on labeled data.\n",
    "- Produces robust and generalized embeddings.\n",
    "- Can be applied across domains (e.g., images, text, multimodal tasks).\n",
    "\n",
    "---\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- **Negative Sampling**: Effective negative sampling is computationally intensive.\n",
    "- **Batch Size**: Large batch sizes are often necessary to include diverse negative samples.\n",
    "- **Mode Collapse**: In some setups, embeddings may collapse, reducing their utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2103.00020v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![image info](.\\CLIP_arhitecture.png \"Title\"){width=10 height=10} -->\n",
    "<img src=\"CLIP_arhitecture.png\" alt=\"CLIP Arhitecture\" class=\"hover-effect\" width=900>\n",
    "\n",
    "Source: <a href=\"https://arxiv.org/abs/2103.00020v1\" > Learning Transferable Visual Models From Natural Language Supervision </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9921, 0.0079]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "texts = [\"A photo of a cat\", \"A photo of a dog\"]\n",
    "image = Image.open(\"cat.jpg\")\n",
    "\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # Image-to-text similarity scores\n",
    "print(logits_per_image.softmax(dim=1))  # Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5380, 0.4620]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "texts2 = [\"A photo of a cat\", \"A photo of a dog\"]\n",
    "image2 = Image.open(\"cat_dog.jpg\")\n",
    "\n",
    "inputs = processor(text=texts2, images=image2, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs2 = model(**inputs)\n",
    "logits_per_image2 = outputs2.logits_per_image  # Image-to-text similarity scores\n",
    "print(logits_per_image2.softmax(dim=1))  # Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Augmenting RAG with Image Retrieval**\n",
    "\n",
    "Incorporating **image retrieval** into a Retrieval-Augmented Generation (RAG) system allows handling:\n",
    "- **Image-based queries.**\n",
    "- **Text-based queries.**\n",
    "- **Multimodal queries** (text + image).\n",
    "\n",
    "This enhancement enables RAG systems to handle richer queries and provide more relevant, context-aware responses.\n",
    "\n",
    "---\n",
    "\n",
    "## **Expanded RAG Architecture with Image Retrieval**\n",
    "\n",
    "### **1. Embedding Creation**\n",
    "- **Text Queries:** Use a **text encoder** (e.g., BERT, GPT) to generate embeddings for user queries.\n",
    "- **Image Queries:** Use an **image encoder** (e.g., CLIP, CNNs, Vision Transformers) to generate embeddings for image queries.\n",
    "- **Shared Embedding Space:** For multimodal systems, encode text and images into a shared embedding space.\n",
    "\n",
    "### **2. Vector Indexing**\n",
    "- Store embeddings for images and associated metadata in a **vector search engine** such as FAISS, Pinecone, or Google Vertex AI Matching Engine.\n",
    "\n",
    "### **3. Query Execution**\n",
    "- Convert the query (text or image) into embeddings and retrieve the top-k most relevant results.\n",
    "- For multimodal queries, combine the embeddings of both text and image inputs.\n",
    "\n",
    "### **4. Contextual Augmentation**\n",
    "- Retrieve relevant image captions, metadata, or associated textual information.\n",
    "- Inject the retrieved context into the input for the generative model.\n",
    "\n",
    "### **5. Answer Generation**\n",
    "- Use a generative model (e.g., GPT, T5) to process the query and retrieved context, producing the final response.\n",
    "\n",
    "---\n",
    "\n",
    "## **How Image Retrieval Enhances RAG**\n",
    "\n",
    "1. **Rich Context:** Retrieved images provide additional context to text-based queries.\n",
    "2. **Multimodal Queries:** Users can submit queries like \"Find similar products to this shoe.\"\n",
    "3. **Cross-Modal Retrieval:** Answer questions such as \"Show me something similar to this painting but in blue.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **Technologies for Image Retrieval in RAG**\n",
    "\n",
    "### **1. Image Encoders**\n",
    "- **CLIP:** Maps images and text into a shared embedding space.\n",
    "- **ResNet:** CNN-based architecture for image feature extraction.\n",
    "- **ViT (Vision Transformers):** Processes images in patches, offering high accuracy for large datasets.\n",
    "\n",
    "### **2. Vector Search Engines**\n",
    "- **FAISS:** Open-source library for nearest neighbor search.\n",
    "- **Pinecone:** Managed vector search service with real-time updates and metadata filtering.\n",
    "- **Google Vertex AI Matching Engine:** A cloud-based vector search system.\n",
    "- **Weaviate:** Open-source vector search platform with native multimodal capabilities.\n",
    "- **Autonomous Database:** Oracle cloud variant\n",
    "\n",
    "### **3. Multimodal Generative Models**\n",
    "- **BLIP:** Combines image understanding with text generation for image-based queries.\n",
    "- **OpenAI GPT Models:** Extended to process image captions or descriptions.\n",
    "- **LLaMA or T5:** Fine-tuned for text generation in multimodal RAG systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Visual Question Answering (VQA) <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "VQA models answer natural language questions about images by reasoning over both modalities.\n",
    "\n",
    "#### Key Concepts\n",
    "- **Image Understanding:** Extract visual features using CNNs or ViTs.\n",
    "- **Text Understanding:** Encode the question using transformers.\n",
    "- **Fusion Mechanism:** Combine image and text embeddings to predict the answer.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Let:\n",
    "- $ Q $: Textual representation of the question.\n",
    "- $ V $: Visual representation of the image.\n",
    "\n",
    "The output is the answer $ A $:\n",
    "$$\n",
    "A = \\text{softmax}(W \\cdot \\text{concat}(f_Q(Q), f_V(V)))\n",
    "$$\n",
    "where $ f_Q $ and $ f_V $ are embedding functions for text and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History of VQA\n",
    "- 2015: Early VQA models used LSTMs for questions and CNNs for images.\n",
    "- 2020: Models like **LXMERT** and **ViLT** introduced attention mechanisms for better fusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LXMERT Architecture\n",
    "- **Image Encoder:** Bottom-up attention mechanism for extracting region-based features.\n",
    "- **Text Encoder:** Transformer architecture for processing questions.\n",
    "- **Cross-Modal Encoder:** Layers of attention to combine image and text representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LXMERT Demo Setup and Example: https://github.com/huggingface/transformers/blob/main/examples/research_projects/lxmert/demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for VQA\n",
    "1. Extract visual features from the image.\n",
    "2. Encode the question into a textual embedding.\n",
    "3. Combine embeddings using attention mechanisms.\n",
    "4. Classify into possible answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/transformers/blob/main/examples/research_projects/lxmert/demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What color is the cat?\n",
      "Answer: orange and white\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from PIL import Image\n",
    "\n",
    "def perform_blip_qa(image_path, question):\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "    model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(image, question, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform inference using generate()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs)\n",
    "    \n",
    "    # Decode the generated answer\n",
    "    answer = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Test the BLIP QA pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"cat.jpg\"\n",
    "    question = \"What color is the cat?\"\n",
    "\n",
    "    # Perform BLIP QA\n",
    "    answer = perform_blip_qa(image_path, question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Multimodal Models\n",
    "1. **E-commerce:** Search for products using text queries.\n",
    "2. **Education:** Interactive learning tools for children.\n",
    "3. **Healthcare:** Analyze and describe medical images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR <a class=\"anchor\" id=\"last-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CLIP: https://arxiv.org/pdf/2103.00020v1\n",
    "- OmniVision multimodal for edge devies: https://levelup.gitconnected.com/omnivision-968m-the-worlds-most-compact-and-smallest-multimodal-vision-language-model-for-edge-ai-4ccd66082bfb\n",
    "- SimCLR : https://arxiv.org/abs/2002.05709\n",
    "- MoCo: https://arxiv.org/abs/1911.05722\n",
    "- LXMERT QA Demo Setup and Example: https://github.com/huggingface/transformers/blob/main/examples/research_projects/lxmert/demo.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_fii_cv_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
